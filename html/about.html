<h2>About</h2>
<p>This is a set of web pages, a blog if you will, on reworking some of neural networks described in Andrew Trask's excellent book "Deep learning".
</p>
<p>What I find particularly noteworthy in Andrew's book is that he takes a algorithmic approach to learning neural networks over the more mathematical approach taken by all the other authors I have read.  Andrew shows that one does not need to be able to derive the backpropagation algorithm ( the central core of all neural networks) from an understating of the chain rule from calculus. While I do not down grade a knowledge of calculus in fully understanding neural networks Andrew shows that an algorithmic description of backpropagation is an excellent starting point to understanding neural networks and that this can from a solid foundation both for moving forward with deep learning libraries and for going back to understand the mathematics.</p>
<p>In the final chapter of his book (16) Andrew lays out 10 further steps to take next in ones neural network education. Step 6 is that you try and reverse engineer a net work from an academic paper, and then see if you can improve on it. I am taking this approach to some of the networks in Andrew's book. I don't think I can improve on them but what I do want to do is revers engineer them so that they are in pure python. In the networks that I recreate I will not use numpy.  </p>

